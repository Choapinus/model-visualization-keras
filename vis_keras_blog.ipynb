{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 109, 89, 30)       2280      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 109, 89, 30)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 54, 44, 30)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 54, 44, 40)        19240     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 54, 44, 40)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 27, 22, 40)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 27, 22, 60)        14460     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27, 22, 60)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 11, 60)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8580)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              35147776  \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 35,191,950\n",
      "Trainable params: 35,191,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Processing filter 0\n",
      "Current loss value: -33.55630874633789\n",
      "Filter 0 processed in 0s\n",
      "Processing filter 1\n",
      "Current loss value: -31.260820388793945\n",
      "Filter 1 processed in 0s\n",
      "Processing filter 2\n",
      "Current loss value: -92.95679473876953\n",
      "Filter 2 processed in 0s\n",
      "Processing filter 3\n",
      "Current loss value: -17.261991500854492\n",
      "Filter 3 processed in 0s\n",
      "Processing filter 4\n",
      "Current loss value: -64.52716064453125\n",
      "Filter 4 processed in 0s\n",
      "Processing filter 5\n",
      "Current loss value: -63.73288345336914\n",
      "Filter 5 processed in 0s\n",
      "Processing filter 6\n",
      "Current loss value: -23.662647247314453\n",
      "Filter 6 processed in 0s\n",
      "Processing filter 7\n",
      "Current loss value: -34.63044738769531\n",
      "Filter 7 processed in 0s\n",
      "Processing filter 8\n",
      "Current loss value: -19.594018936157227\n",
      "Filter 8 processed in 0s\n",
      "Processing filter 9\n",
      "Current loss value: -36.10387420654297\n",
      "Filter 9 processed in 0s\n",
      "Processing filter 10\n",
      "Current loss value: -26.87133026123047\n",
      "Filter 10 processed in 0s\n",
      "Processing filter 11\n",
      "Current loss value: -73.00286865234375\n",
      "Filter 11 processed in 0s\n",
      "Processing filter 12\n",
      "Current loss value: -22.747407913208008\n",
      "Filter 12 processed in 0s\n",
      "Processing filter 13\n",
      "Current loss value: -27.03105354309082\n",
      "Filter 13 processed in 0s\n",
      "Processing filter 14\n",
      "Current loss value: -27.6597843170166\n",
      "Filter 14 processed in 0s\n",
      "Processing filter 15\n",
      "Current loss value: -16.856325149536133\n",
      "Filter 15 processed in 0s\n",
      "Processing filter 16\n",
      "Current loss value: -22.20691680908203\n",
      "Filter 16 processed in 0s\n",
      "Processing filter 17\n",
      "Current loss value: -23.12200164794922\n",
      "Filter 17 processed in 0s\n",
      "Processing filter 18\n",
      "Current loss value: -19.89671516418457\n",
      "Filter 18 processed in 0s\n",
      "Processing filter 19\n",
      "Current loss value: -24.969175338745117\n",
      "Filter 19 processed in 0s\n",
      "Processing filter 20\n",
      "Current loss value: -23.866941452026367\n",
      "Filter 20 processed in 0s\n",
      "Processing filter 21\n",
      "Current loss value: -82.7270736694336\n",
      "Filter 21 processed in 0s\n",
      "Processing filter 22\n",
      "Current loss value: -41.854740142822266\n",
      "Filter 22 processed in 0s\n",
      "Processing filter 23\n",
      "Current loss value: -40.92165756225586\n",
      "Filter 23 processed in 0s\n",
      "Processing filter 24\n",
      "Current loss value: -65.59314727783203\n",
      "Filter 24 processed in 0s\n",
      "Processing filter 25\n",
      "Current loss value: -15.2367525100708\n",
      "Filter 25 processed in 0s\n",
      "Processing filter 26\n",
      "Current loss value: -47.845787048339844\n",
      "Filter 26 processed in 0s\n",
      "Processing filter 27\n",
      "Current loss value: -56.16877365112305\n",
      "Filter 27 processed in 0s\n",
      "Processing filter 28\n",
      "Current loss value: -24.40740203857422\n",
      "Filter 28 processed in 0s\n",
      "Processing filter 29\n",
      "Current loss value: -20.149255752563477\n",
      "Filter 29 processed in 1s\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "# from keras.preprocessing.image import save_img\n",
    "from keras.applications import vgg16\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "\n",
    "# dimensions of the generated pictures for each filter.\n",
    "# img_width = 128\n",
    "# img_height = 128\n",
    "img_width = 109\n",
    "img_height = 89\n",
    "\n",
    "# the name of the layer we want to visualize\n",
    "# (see model definition at keras/applications/vgg16.py)\n",
    "layer_name = 'block5_conv1'\n",
    "# layer_name = 'conv2d_3'\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "# build the VGG16 network with ImageNet weights\n",
    "model = vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "# model = load_model('../male_female_cnn/santa_not_santa.model')\n",
    "\n",
    "print('Model loaded.')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# this is the placeholder for the input images\n",
    "input_img = model.input\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n",
    "\n",
    "\n",
    "kept_filters = []\n",
    "for filter_index in range(30):\n",
    "    # we only scan through the first 200 filters,\n",
    "    # but there are actually 512 of them\n",
    "    print('Processing filter %d' % filter_index)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # we build a loss function that maximizes the activation\n",
    "    # of the nth filter of the layer considered\n",
    "    layer_output = layer_dict[layer_name].output\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "    else:\n",
    "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    # we compute the gradient of the input picture wrt this loss\n",
    "    grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "    # normalization trick: we normalize the gradient\n",
    "    grads = normalize(grads)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "    # step size for gradient ascent\n",
    "    step = 1.\n",
    "\n",
    "    # we start from a gray image with some random noise\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_img_data = np.random.random((1, 3, img_width, img_height))\n",
    "    else:\n",
    "        input_img_data = np.random.random((1, img_width, img_height, 3))\n",
    "    input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "    # we run gradient ascent for 20 steps\n",
    "    for i in range(20):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "\n",
    "        print('Current loss value:', loss_value)\n",
    "        if loss_value <= 0.:\n",
    "            # some filters get stuck to 0, we can skip them\n",
    "            break\n",
    "\n",
    "    # decode the resulting input image\n",
    "    if loss_value > 0:\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        kept_filters.append((img, loss_value))\n",
    "    end_time = time.time()\n",
    "    print('Filter %d processed in %ds' % (filter_index, end_time - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-7750ab21ab04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkept_filters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         stitched_filters[(img_width + margin) * i: (img_width + margin) * i + img_width,\n\u001b[1;32m     21\u001b[0m                          (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# we will stich the best 64 filters on a 8 x 8 grid.\n",
    "n = 8\n",
    "\n",
    "# the filters that have the highest loss are assumed to be better-looking.\n",
    "# we will only keep the top 64 filters.\n",
    "kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "kept_filters = kept_filters[:n * n]\n",
    "\n",
    "# build a black picture with enough space for\n",
    "# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "margin = 5\n",
    "width = n * img_width + (n - 1) * margin\n",
    "height = n * img_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "# fill the picture with our saved filters\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        img, loss = kept_filters[i * n + j]\n",
    "        stitched_filters[(img_width + margin) * i: (img_width + margin) * i + img_width,\n",
    "                         (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img\n",
    "\n",
    "# save the result to disk\n",
    "# save_img('stitched_filters_%dx%d.png' % (n, n), stitched_filters)\n",
    "cv2.imwrite('stitched_filters_%dx%d.png' % (n, n), stitched_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_class_activation_map(model_path, img_path, output_path):\n",
    "    model = load_model(model_path)\n",
    "    original_img = cv2.imread(img_path, 1)\n",
    "    width, height, _ = original_img.shape\n",
    "\n",
    "    #Reshape to the network input shape (3, w, h).\n",
    "    img = np.array([np.transpose(np.float32(original_img), (2, 0, 1))])\n",
    "\n",
    "    #Get the 512 input weights to the softmax.\n",
    "    class_weights = model.layers[-1].get_weights()[0]\n",
    "    final_conv_layer = get_output_layer(model, \"conv5_3\")\n",
    "    get_output = K.function([model.layers[0].input], \\\n",
    "                [final_conv_layer.output, \n",
    "    model.layers[-1].output])\n",
    "    [conv_outputs, predictions] = get_output([img])\n",
    "    conv_outputs = conv_outputs[0, :, :, :]\n",
    "\n",
    "    #Create the class activation map.\n",
    "    cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[1:3])\n",
    "    target_class = 1\n",
    "    for i, w in enumerate(class_weights[:, target_class]):\n",
    "            cam += w * conv_outputs[i, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_output_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-883d8e9c24f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m visualize_class_activation_map('../male_female_cnn/santa_not_santa.model', \n\u001b[1;32m      2\u001b[0m                                \u001b[0;34m'../male_female_cnn/examples/dave.jpeg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                               '../male_female_cnn/examples/_dave.jpeg')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-710d41ba9a6d>\u001b[0m in \u001b[0;36mvisualize_class_activation_map\u001b[0;34m(model_path, img_path, output_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#Get the 512 input weights to the softmax.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfinal_conv_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_output_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"conv5_3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     get_output = K.function([model.layers[0].input],                 [final_conv_layer.output, \n\u001b[1;32m     13\u001b[0m     model.layers[-1].output])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_output_layer' is not defined"
     ]
    }
   ],
   "source": [
    "visualize_class_activation_map('../male_female_cnn/santa_not_santa.model', \n",
    "                               '../male_female_cnn/examples/dave.jpeg',\n",
    "                              '../male_female_cnn/examples/_dave.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
